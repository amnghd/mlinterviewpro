{"type":"excalidraw","version":2,"source":"https://app.excalidraw.com","metadata":{"id":"8Wr5V5oxqPL","workspace":"6lsqOkFWqjK","name":"AIrBNB Search Retraival","created":"2025-10-29T17:54:50.079Z","updated":"2025-10-29T18:23:02.578Z","collection":null,"creator":"Alx6QpYT9Vj","updater":"Alx6QpYT9Vj","isPrivate":true,"pinned":false},"elements":[{"id":"LtAdxMC2W0Zogmf79kPDs","type":"text","x":556.87890625,"y":348,"width":1134.920654296875,"height":1230.0501802884614,"angle":0,"strokeColor":"#1e1e1e","backgroundColor":"transparent","fillStyle":"solid","strokeWidth":2,"strokeStyle":"solid","roughness":1,"opacity":100,"groupIds":[],"frameId":null,"index":"a0","roundness":null,"seed":701314716,"version":2172,"versionNonce":183153188,"isDeleted":false,"boundElements":null,"updated":1761762177722,"link":null,"locked":false,"text":"Airbnb Search Retrieval\n\n- should we cover large geo area California, France, ...\n- allowing time based search?\n- multi lingual?\n- latency 30-40ms? \n- filters: geo, price, capacity\n- evaluation: offline recall @ k, booking lift\n- ANN choice constraint: real-time updates, filter latency, memory footprint\n\n\nHigh level system:\n- convert query and listing to numerical vectors?\n- two tower embedding based retrival (to k ranker)\n- Recall as the main metric\n- MRR (given one is chosen for booking) so recall @ k with k == 1 always\n- filter step removing targetting range\n\n\nData construction\n- Define trip: location, guests, length of stay => group user's historical searches lead to booking\n\n- Labels:\n-- positive, negative\n--- positive: booked listing\n--- negative: user interacted with in search results (impression) (maybe even bookmarking)\n--- and not eventually converted (hard negatives)\n\n\n- feature engineering\n-- query: geo intent, guests, LOS, date flexiblity, filters, langageues, devices, user embdeding, user cluster\n-- listing: amenities (one hot), capacity, histoical engagement, host features, price band, location cell, quality signals\n-- loss: sampled softmax positives, and in journey hard (book-marked, not converted) negatives\n\nModel architecture\n- \n- listing done offline and precomputed\n- Inverted file index (IVF: cluster centroid) better than HNSW because of lower memory footprint\n- Choice of similarity (dot product, euclidian) is important. \n\n\nModel training\n- contrastive loss (positive, and hard in session negative)\n- we should also retrain a ranker on top of it\n\nEvaluation:\n-- offline: MRR, recall @k (if we care about multiple, e.g. bookmark), diversity,\n-- online: booking lift (sliced), reported stat sign booking gain from EBR => \n--","fontSize":20.082451923076924,"fontFamily":5,"textAlign":"left","verticalAlign":"top","containerId":null,"originalText":"Airbnb Search Retrieval\n\n- should we cover large geo area California, France, ...\n- allowing time based search?\n- multi lingual?\n- latency 30-40ms? \n- filters: geo, price, capacity\n- evaluation: offline recall @ k, booking lift\n- ANN choice constraint: real-time updates, filter latency, memory footprint\n\n\nHigh level system:\n- convert query and listing to numerical vectors?\n- two tower embedding based retrival (to k ranker)\n- Recall as the main metric\n- MRR (given one is chosen for booking) so recall @ k with k == 1 always\n- filter step removing targetting range\n\n\nData construction\n- Define trip: location, guests, length of stay => group user's historical searches lead to booking\n\n- Labels:\n-- positive, negative\n--- positive: booked listing\n--- negative: user interacted with in search results (impression) (maybe even bookmarking)\n--- and not eventually converted (hard negatives)\n\n\n- feature engineering\n-- query: geo intent, guests, LOS, date flexiblity, filters, langageues, devices, user embdeding, user cluster\n-- listing: amenities (one hot), capacity, histoical engagement, host features, price band, location cell, quality signals\n-- loss: sampled softmax positives, and in journey hard (book-marked, not converted) negatives\n\nModel architecture\n- \n- listing done offline and precomputed\n- Inverted file index (IVF: cluster centroid) better than HNSW because of lower memory footprint\n- Choice of similarity (dot product, euclidian) is important. \n\n\nModel training\n- contrastive loss (positive, and hard in session negative)\n- we should also retrain a ranker on top of it\n\nEvaluation:\n-- offline: MRR, recall @k (if we care about multiple, e.g. bookmark), diversity,\n-- online: booking lift (sliced), reported stat sign booking gain from EBR => \n--","autoResize":true,"lineHeight":1.25}],"appState":{"viewBackgroundColor":"#ffffff","lockedMultiSelections":{}},"files":{}}