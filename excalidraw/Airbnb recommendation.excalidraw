{"type":"excalidraw","version":2,"source":"https://app.excalidraw.com","metadata":{"id":"7GCxsAVePMI","workspace":"6lsqOkFWqjK","name":"Airbnb recommendation","created":"2025-10-29T21:28:49.594Z","updated":"2025-10-29T21:39:21.746Z","collection":null,"creator":"Alx6QpYT9Vj","updater":"Alx6QpYT9Vj","isPrivate":true,"pinned":false},"elements":[{"id":"JRYYbbV6FEIqiK6RCMwou","type":"text","x":431,"y":377.734375,"width":1108.55908203125,"height":5300,"angle":0,"strokeColor":"#1e1e1e","backgroundColor":"transparent","fillStyle":"solid","strokeWidth":2,"strokeStyle":"solid","roughness":1,"opacity":100,"groupIds":[],"frameId":null,"index":"a0","roundness":null,"seed":429498396,"version":801,"versionNonce":1091180964,"isDeleted":false,"boundElements":[],"updated":1761773954013,"link":null,"locked":false,"text":"Improving Airbnb’s Recommendation Algorithm for Guests\n\nCQ\n\nWhat part of funnel? Discovery feed (homepage / search suggestions) or booking page?\n\nKPI: conversion rate (booking), guest engagement (wishlist, view, share), host exposure fairness?\n\nGuardrails: complaint rate, bounce rate after click, diversity of listings, latency (<150 ms p99).\n\nScope: multi-lingual, multi-country, large catalogue (100 M + listings).\n\nFeatures: historical guest-host interactions, amenities, price, location, review embeddings.\n\nCold start for new guests/listings? yes.\n\nRanking vs retrieval separation? yes — retrieval (relevance recall), ranker (precision).\n\nReal-time constraints: dynamic price, availability, cancellation, message exchanges.\n\nHigh Level\n\nGoal: maximize booking probability per impression while maintaining diversity, freshness, and fairness across hosts.\n\nPipeline:\n\nCandidate Generation (Retrieval) → reduce space from O(100 M) → O(1 K).\n\nRanking Model → optimize multi-task engagement (click, wishlist, booking).\n\nReranker → inject freshness, diversity, fairness.\n\nMulti-objective optimization (MMOE) for engagement tasks (CTR, booking, dwell time).\n\nOnline continual learning to adapt to new listings and user trends.\n\nData Construction\n\nLabeling\n\nPositive: booked or added to wishlist.\n\nSecondary positives: clicked, viewed > 30 s, shared.\n\nNegative: impressions with no engagement.\n\nWeighted label: 0.6 × book + 0.2 × wishlist + 0.2 × click.\n\nTemporal validation (train Q1 → test Q2 to avoid leakage).\n\nDownsample negatives 10× for class balance.\n\nFeature Engineering\n\nUser features\n\nuser_id embedding; demographics (age, country, device cluster).\n\nTravel history (previous destinations, seasonality patterns, price band clusters).\n\nStay intent (short/long term, leisure vs business BERT embedding from query/message).\n\nuser_active_days, avg_review_left, preferred_property_type.\n\nAggregated embeddings of past booked/listed IDs (sequence model GRU).\n\nListing features\n\nlisting_id embedding (learned).\n\nTextual description → DistilBERT embedding (title + amenities).\n\nLocation: lat/long (bucketized), walkability, POI density, distance to attractions.\n\nHost features (host rating, host response time, host cancel rate).\n\nPrice (log-scaled, bucketized), seasonal discounts, availability score.\n\nVisual embeddings (SimCLR/CLIP for images → store in feature store).\n\nContext / request features\n\nDevice, session language, geo, local time, DOW/TOD, holiday flag.\n\nQuery intent (search keywords) → semantic embedding.\n\nStay lead time, length of stay.\n\nLatency budget: feature hydration < 50 ms.\n\nCross features\n\nUser–Listing affinity (price match, amenity overlap, location distance).\n\nUser–Host history (past bookings, rating correlation).\n\nTemporal features (recency decay of interaction).\n\nModeling Architecture\nRetrieval (Recall stage)\n\nTwo-tower model (user tower + listing tower).\n\nInput: user embeddings (behavioral, context) vs listing embeddings (text, image, metadata).\n\nLoss: contrastive softmax (loss = – log P (positive listing | user)).\n\nANN index (SCANN/FAISS) → O(log n) retrieval.\n\nContinual training hourly with incremental embeddings.\n\nRanking model\n\nSingle-tower deep DNN (X-features crossed via DCNv2).\n\nTasks: click (head 1), wishlist (head 2), booking (head 3).\n\nMMOE (gated experts per task to avoid negative transfer).\n\nLoss = weighted sum (BCE_task × α_task) with α = [0.2, 0.3, 0.5].\n\nFocal loss to up-weight rare bookings.\n\nCalibration layer (Platt scaling → isotonic per region).\n\nReranker\n\nInject freshness (new listings boost).\n\nDiversity penalty on cosine similarity of listing embeddings.\n\nFairness penalty on host exposure imbalance.\n\nLightweight MLP that re-scores top N (50 – 100) candidates.\n\nTraining\n\nNegative downsampling (10:1), temporal split (avoid leakage).\n\nGradient blending to stabilize MMOE training.\n\nRegularization: dropout (0.2 – 0.5), weight decay.\n\nContinual learning jobs daily + event-driven (triggered by listing additions or policy updates).\n\nEvaluation\n\nOffline\n\nRanking: NDCG@k (weighted by booking relevance).\n\nAUC for binary tasks (click, wishlist, book).\n\nCalibration quality (Brier score, ECE).\n\nDiversity score (mean pairwise cosine distance on top k).\n\nOnline\n\nBooking rate ↑ (CTR → booking conversion chain).\n\nWishlist rate ↑, DAU ↑, average revenue per session ↑.\n\nGuardrails: complaint rate, CS contact rate, latency < 150 ms.\n\nServing\n\nStage 1: Retrieval (Two-tower ANN → Top 1 K).\n\nStage 2: Heavy ranker (DNN with cross features → Top 100).\n\nStage 3: Reranker (Diversity, Freshness, Fairness → Top 20).\n\nEmbedding quantization (AQT) + cache (geo + user segment).\n\nStreaming feature updates (payment status, price change).\n\nModel latency budget split: Retrieval 30 ms, Ranker 70 ms, Reranker 30 ms.\n\nDeployment & Monitoring\n\nShadow current ranking model (offline pred distribution sanity).\n\nCanary 1% traffic → monitor guardrails.\n\nA/B test with multi-arm bandit for exploration/exploitation balance.\n\nReal-time metrics: latency, failure rate, feature staleness < 5%.\n\nModel drift monitoring (PSI, KS test on top features).\n\nBias / Fairness / Cold Start\n\nBias control: popularity penalty in reranker + ε-greedy exploration (ε = 0.05).\n\nCold start listing: use content-based tower (text + image embeddings) for new items.\n\nCold start guest: cluster-based priors + country/season baseline model.\n\nFairness: ensure exposure parity by host and region (weighted sampling during training).\n\nAdvanced Improvements\n\nSequence model for guest journey (long-term embedding via Transformer).\n\nContextual bandit for exploration (new listings injection).\n\nLTV head (predicted lifetime booking value → personalized ranking).\n\nMultimodal fusion (model co-train on images + text description + reviews for robust representation).\n\nCausal uplift model to detect which recommendations increase booking probability versus confounding correlation.","fontSize":20,"fontFamily":5,"textAlign":"left","verticalAlign":"top","containerId":null,"originalText":"Improving Airbnb’s Recommendation Algorithm for Guests\n\nCQ\n\nWhat part of funnel? Discovery feed (homepage / search suggestions) or booking page?\n\nKPI: conversion rate (booking), guest engagement (wishlist, view, share), host exposure fairness?\n\nGuardrails: complaint rate, bounce rate after click, diversity of listings, latency (<150 ms p99).\n\nScope: multi-lingual, multi-country, large catalogue (100 M + listings).\n\nFeatures: historical guest-host interactions, amenities, price, location, review embeddings.\n\nCold start for new guests/listings? yes.\n\nRanking vs retrieval separation? yes — retrieval (relevance recall), ranker (precision).\n\nReal-time constraints: dynamic price, availability, cancellation, message exchanges.\n\nHigh Level\n\nGoal: maximize booking probability per impression while maintaining diversity, freshness, and fairness across hosts.\n\nPipeline:\n\nCandidate Generation (Retrieval) → reduce space from O(100 M) → O(1 K).\n\nRanking Model → optimize multi-task engagement (click, wishlist, booking).\n\nReranker → inject freshness, diversity, fairness.\n\nMulti-objective optimization (MMOE) for engagement tasks (CTR, booking, dwell time).\n\nOnline continual learning to adapt to new listings and user trends.\n\nData Construction\n\nLabeling\n\nPositive: booked or added to wishlist.\n\nSecondary positives: clicked, viewed > 30 s, shared.\n\nNegative: impressions with no engagement.\n\nWeighted label: 0.6 × book + 0.2 × wishlist + 0.2 × click.\n\nTemporal validation (train Q1 → test Q2 to avoid leakage).\n\nDownsample negatives 10× for class balance.\n\nFeature Engineering\n\nUser features\n\nuser_id embedding; demographics (age, country, device cluster).\n\nTravel history (previous destinations, seasonality patterns, price band clusters).\n\nStay intent (short/long term, leisure vs business BERT embedding from query/message).\n\nuser_active_days, avg_review_left, preferred_property_type.\n\nAggregated embeddings of past booked/listed IDs (sequence model GRU).\n\nListing features\n\nlisting_id embedding (learned).\n\nTextual description → DistilBERT embedding (title + amenities).\n\nLocation: lat/long (bucketized), walkability, POI density, distance to attractions.\n\nHost features (host rating, host response time, host cancel rate).\n\nPrice (log-scaled, bucketized), seasonal discounts, availability score.\n\nVisual embeddings (SimCLR/CLIP for images → store in feature store).\n\nContext / request features\n\nDevice, session language, geo, local time, DOW/TOD, holiday flag.\n\nQuery intent (search keywords) → semantic embedding.\n\nStay lead time, length of stay.\n\nLatency budget: feature hydration < 50 ms.\n\nCross features\n\nUser–Listing affinity (price match, amenity overlap, location distance).\n\nUser–Host history (past bookings, rating correlation).\n\nTemporal features (recency decay of interaction).\n\nModeling Architecture\nRetrieval (Recall stage)\n\nTwo-tower model (user tower + listing tower).\n\nInput: user embeddings (behavioral, context) vs listing embeddings (text, image, metadata).\n\nLoss: contrastive softmax (loss = – log P (positive listing | user)).\n\nANN index (SCANN/FAISS) → O(log n) retrieval.\n\nContinual training hourly with incremental embeddings.\n\nRanking model\n\nSingle-tower deep DNN (X-features crossed via DCNv2).\n\nTasks: click (head 1), wishlist (head 2), booking (head 3).\n\nMMOE (gated experts per task to avoid negative transfer).\n\nLoss = weighted sum (BCE_task × α_task) with α = [0.2, 0.3, 0.5].\n\nFocal loss to up-weight rare bookings.\n\nCalibration layer (Platt scaling → isotonic per region).\n\nReranker\n\nInject freshness (new listings boost).\n\nDiversity penalty on cosine similarity of listing embeddings.\n\nFairness penalty on host exposure imbalance.\n\nLightweight MLP that re-scores top N (50 – 100) candidates.\n\nTraining\n\nNegative downsampling (10:1), temporal split (avoid leakage).\n\nGradient blending to stabilize MMOE training.\n\nRegularization: dropout (0.2 – 0.5), weight decay.\n\nContinual learning jobs daily + event-driven (triggered by listing additions or policy updates).\n\nEvaluation\n\nOffline\n\nRanking: NDCG@k (weighted by booking relevance).\n\nAUC for binary tasks (click, wishlist, book).\n\nCalibration quality (Brier score, ECE).\n\nDiversity score (mean pairwise cosine distance on top k).\n\nOnline\n\nBooking rate ↑ (CTR → booking conversion chain).\n\nWishlist rate ↑, DAU ↑, average revenue per session ↑.\n\nGuardrails: complaint rate, CS contact rate, latency < 150 ms.\n\nServing\n\nStage 1: Retrieval (Two-tower ANN → Top 1 K).\n\nStage 2: Heavy ranker (DNN with cross features → Top 100).\n\nStage 3: Reranker (Diversity, Freshness, Fairness → Top 20).\n\nEmbedding quantization (AQT) + cache (geo + user segment).\n\nStreaming feature updates (payment status, price change).\n\nModel latency budget split: Retrieval 30 ms, Ranker 70 ms, Reranker 30 ms.\n\nDeployment & Monitoring\n\nShadow current ranking model (offline pred distribution sanity).\n\nCanary 1% traffic → monitor guardrails.\n\nA/B test with multi-arm bandit for exploration/exploitation balance.\n\nReal-time metrics: latency, failure rate, feature staleness < 5%.\n\nModel drift monitoring (PSI, KS test on top features).\n\nBias / Fairness / Cold Start\n\nBias control: popularity penalty in reranker + ε-greedy exploration (ε = 0.05).\n\nCold start listing: use content-based tower (text + image embeddings) for new items.\n\nCold start guest: cluster-based priors + country/season baseline model.\n\nFairness: ensure exposure parity by host and region (weighted sampling during training).\n\nAdvanced Improvements\n\nSequence model for guest journey (long-term embedding via Transformer).\n\nContextual bandit for exploration (new listings injection).\n\nLTV head (predicted lifetime booking value → personalized ranking).\n\nMultimodal fusion (model co-train on images + text description + reviews for robust representation).\n\nCausal uplift model to detect which recommendations increase booking probability versus confounding correlation.","autoResize":true,"lineHeight":1.25}],"appState":{"viewBackgroundColor":"#ffffff","lockedMultiSelections":{}},"files":{}}